{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e231a7",
   "metadata": {},
   "source": [
    "# CV18. 프로젝트 : 스티커를 붙여주자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53838630",
   "metadata": {},
   "source": [
    "## 0. 필요한 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f680ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import tqdm\n",
    "import time\n",
    "global load_t1\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95353d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "WARNING:tensorflow:From <ipython-input-2-f4a452a32625>:8: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "# 2.3.0\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca13fd3",
   "metadata": {},
   "source": [
    "## 1. WIDER FACE 데이터셋 전처리(1)  분석\n",
    "\n",
    "\n",
    "* Bounding box 정보 : \n",
    "  - wider_face_train_bbx_gt.txt, \n",
    "  - wider_face_val_bbx_gt.txt  파일에 있음.\n",
    "  \n",
    "  \n",
    "* face bounding box 좌표 등 상세정보 (10개의 숫자):\n",
    "  - x0, y0, w, h, blur, expression, illumination, invalid, occlusion, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1887f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box 정보(좌상 꼭지점 X좌표, Y좌표, 너비, 높이)\n",
    "def get_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8826eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지별 bounding box 정보를 wider_face_train_bbx_gt.txt에서 파싱해서 리스트로 추출\n",
    "def parse_widerface(config_path):\n",
    "    boxes_per_img = []\n",
    "    with open(config_path) as fp:\n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            num_of_obj = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(num_of_obj):\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                if w == 0:\n",
    "                    # remove boxes with no width\n",
    "                    continue\n",
    "                if h == 0:\n",
    "                    # remove boxes with no height\n",
    "                    continue\n",
    "                # Because our network is outputting 7x7 grid then it's not worth processing images with more than\n",
    "                # 5 faces because it's highly probable they are close to each other.\n",
    "                # You could remove this filter if you decide to switch to larger grid (like 14x14)\n",
    "                # Don't worry about number of train data because even with this filter we have around 16k samples\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if num_of_obj == 0:\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            boxes_per_img.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "            cnt += 1\n",
    "\n",
    "    return boxes_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e981429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_file):\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        logging.info('{}: Invalid JPEG data or crop window'.format(image_file))\n",
    "        return 1, image_string, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87a284",
   "metadata": {},
   "source": [
    "* `tf.io.read_file()` :\n",
    "* `tf.image.decode_jpeg()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5946298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boudbing box 정보를 꼭지점 좌표 정보로 변환\n",
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "    \n",
    "    # [x, y, w, h] -> x_min, y_min, x_max, y_max\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b8ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'filename': '/home/aiffel-d34j/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_849.jpg', 'width': 1024, 'height': 1385, 'depth': 3, 'class': [1], 'xmin': [449], 'ymin': [330], 'xmax': [571], 'ymax': [479], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel-d34j/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_Parade_0_904.jpg', 'width': 1024, 'height': 1432, 'depth': 3, 'class': [1], 'xmin': [361], 'ymin': [98], 'xmax': [624], 'ymax': [437], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel-d34j/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_799.jpg', 'width': 1024, 'height': 768, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [78, 78, 113, 134, 163, 201, 182, 245, 304, 328, 389, 406, 436, 522, 643, 653, 793, 535, 29, 3, 20], 'ymin': [221, 238, 212, 260, 250, 218, 266, 279, 265, 295, 281, 293, 290, 328, 320, 224, 337, 311, 220, 232, 215], 'xmax': [85, 92, 124, 149, 177, 211, 197, 263, 320, 344, 406, 427, 458, 543, 666, 670, 816, 551, 40, 14, 32], 'ymax': [229, 255, 227, 275, 267, 230, 283, 294, 282, 315, 300, 314, 307, 346, 342, 249, 367, 328, 235, 247, 231], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel-d34j/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_117.jpg', 'width': 1024, 'height': 682, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [69, 227, 296, 353, 885, 819, 727, 598, 740], 'ymin': [359, 382, 305, 280, 377, 391, 342, 246, 308], 'xmax': [119, 283, 340, 393, 948, 853, 764, 631, 785], 'ymax': [395, 425, 331, 316, 418, 434, 373, 275, 341], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel-d34j/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_778.jpg', 'width': 1024, 'height': 852, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [27, 63, 64, 88, 231, 263, 367, 198, 293, 412, 441, 475, 510, 576, 577, 595, 570, 645, 719, 791, 884, 898, 945, 922, 743, 841, 980, 1001, 488, 586, 669, 744, 803, 294, 203], 'ymin': [226, 95, 63, 13, 1, 122, 68, 98, 161, 36, 23, 40, 23, 30, 71, 94, 126, 171, 98, 154, 97, 48, 89, 38, 71, 18, 56, 107, 2, 1, 1, 2, 3, 2, 0], 'xmax': [60, 79, 81, 104, 244, 277, 382, 213, 345, 426, 458, 489, 524, 592, 593, 611, 583, 697, 730, 845, 900, 913, 960, 937, 754, 857, 993, 1015, 500, 601, 681, 762, 821, 305, 216], 'ymax': [262, 114, 81, 28, 14, 142, 91, 116, 220, 56, 36, 61, 40, 45, 92, 114, 142, 229, 113, 203, 118, 69, 109, 54, 89, 34, 76, 120, 20, 18, 16, 17, 20, 12, 14], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.getenv('HOME')+'/aiffel/face_detector/widerface'\n",
    "anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "file_path = 'WIDER_train'\n",
    "\n",
    "for i, info in enumerate(parse_widerface(os.path.join(dataset_path, 'wider_face_split', anno_txt))):\n",
    "    print('--------------------')\n",
    "    image_file = os.path.join(dataset_path, file_path, 'images', info[0])\n",
    "    error, image_string, image_data = process_image(image_file)\n",
    "    boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "    print(boxes)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ceb40",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 전처리(2) tf_example 생성\n",
    "\n",
    "\n",
    "### tfrecord 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edbb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개 데이터의 단위를 이루는 `tf.train.Example` 인스턴스를 생성하는 메소드\n",
    "def make_example(image_string, image_info_list):\n",
    "\n",
    "    for info in image_info_list:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename' :tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height'   :tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width'    :tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes'  :tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins'   :tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins'   :tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes'  :tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes'  :tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31388268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12880/12880 [00:44<00:00, 291.67it/s]\n",
      "100%|██████████| 3226/3226 [00:10<00:00, 301.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# example을 serialize하여 바이너리 파일로 생성\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "rootPath = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "dataset_path = 'widerface'\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    logging.info('Please define valid dataset path.')\n",
    "else:\n",
    "    logging.info('Loading {}'.format(dataset_path))\n",
    "\n",
    "logging.info('Reading configuration...')\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    output_file = rootPath + '/dataset/train_mask.tfrecord' if split == 'train' else rootPath + '/dataset/val_mask.tfrecord'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "\n",
    "        counter = 0\n",
    "        skipped = 0\n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt' if split == 'train' else 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train' if split == 'train' else 'WIDER_val'\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(rootPath, dataset_path, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(rootPath, dataset_path, file_path, 'images', info[0])\n",
    "\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "                counter += 1\n",
    "\n",
    "            else:\n",
    "                skipped += 1\n",
    "                logging.info('Skipped {:d} of {:d} images.'.format(skipped, counter))\n",
    "\n",
    "    logging.info('Wrote {} images to {}'.format(counter, output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625960c",
   "metadata": {},
   "source": [
    "* 전처리 모듈 : `tf_dataset_preprocess.py`   \n",
    "* 전처리 실행 : ` cd ~/aiffel/face_detector && python tf_dataset_preprocess.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9bf1bf",
   "metadata": {},
   "source": [
    "## 3. 모델 구현(1) priors box\n",
    "\n",
    "### config 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb1dd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'input_size': (240, 320),\n",
       " 'dataset_path': 'dataset/train_mask.tfrecord',\n",
       " 'val_path': 'dataset/val_mask.tfrecord',\n",
       " 'dataset_len': 12880,\n",
       " 'val_len': 3226,\n",
       " 'using_crop': True,\n",
       " 'using_bin': True,\n",
       " 'using_flip': True,\n",
       " 'using_distort': True,\n",
       " 'using_normalizing': True,\n",
       " 'labels_list': ['background', 'face'],\n",
       " 'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
       " 'steps': [8, 16, 32, 64],\n",
       " 'match_thresh': 0.45,\n",
       " 'variances': [0.1, 0.2],\n",
       " 'clip': False,\n",
       " 'base_channel': 16,\n",
       " 'resume': False,\n",
       " 'epoch': 100,\n",
       " 'init_lr': 0.001,\n",
       " 'lr_decay_epoch': [5, 7],\n",
       " 'lr_rate': 0.001,\n",
       " 'warmup_epoch': 3,\n",
       " 'min_lr': 1e-05,\n",
       " 'weights_decay': 0.0005,\n",
       " 'momentum': 0.9,\n",
       " 'save_freq': 10,\n",
       " 'score_threshold': 0.5,\n",
       " 'nms_threshold': 0.4,\n",
       " 'max_number_keep': 200}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "    # general setting\n",
    "    \"batch_size\": 32,\n",
    "    \"input_size\": (240, 320),  # (h,w)\n",
    "\n",
    "    # training dataset\n",
    "    \"dataset_path\": 'dataset/train_mask.tfrecord',  # 'dataset/trainval_mask.tfrecord'\n",
    "    \"val_path\": 'dataset/val_mask.tfrecord',  #\n",
    "    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\n",
    "    \"val_len\": 3226,\n",
    "    \"using_crop\": True,\n",
    "    \"using_bin\": True,\n",
    "    \"using_flip\": True,\n",
    "    \"using_distort\": True,\n",
    "    \"using_normalizing\": True,\n",
    "    \"labels_list\": ['background', 'face'],  # xml annotation\n",
    "\n",
    "    # anchor setting\n",
    "    \"min_sizes\":[[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
    "    \"steps\": [8, 16, 32, 64],\n",
    "    \"match_thresh\": 0.45,\n",
    "    \"variances\": [0.1, 0.2],\n",
    "    \"clip\": False,\n",
    "\n",
    "    # network\n",
    "    \"base_channel\": 16,\n",
    "\n",
    "    # training setting\n",
    "    \"resume\": False,   # if False,training from scratch\n",
    "    \"epoch\": 100,       # 100\n",
    "    \"init_lr\": 1e-3,   # 1e-2\n",
    "    \"lr_decay_epoch\": [5, 7],   #[50, 70]\n",
    "    \"lr_rate\": 0.001,       # 0.1\n",
    "    \"warmup_epoch\": 3,     # 5\n",
    "    \"min_lr\": 1e-5,        # 1e-4\n",
    "\n",
    "    \"weights_decay\": 5e-4,\n",
    "    \"momentum\": 0.9, \n",
    "    \"save_freq\": 10, #frequency of save model weights\n",
    "\n",
    "    # inference\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"nms_threshold\": 0.4,\n",
    "    \"max_number_keep\": 200\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ac245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320)\n"
     ]
    }
   ],
   "source": [
    "# config 중 prior(anchor) box 생성과 관련된 것들은 다음과 같습니다.\n",
    "image_sizes = cfg['input_size']\n",
    "min_sizes = cfg[\"min_sizes\"]\n",
    "steps = cfg[\"steps\"]\n",
    "clip = cfg[\"clip\"]\n",
    "\n",
    "if isinstance(image_sizes, int):\n",
    "    image_sizes = (image_sizes, image_sizes)\n",
    "elif isinstance(image_sizes, tuple):\n",
    "    image_sizes = image_sizes\n",
    "else:\n",
    "    raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "print(image_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654477c8",
   "metadata": {},
   "source": [
    "* `isinstance()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356b959",
   "metadata": {},
   "source": [
    "<img align='center' width='80%' src='https://aiffelstaticprd.blob.core.windows.net/media/images/GC-11-P-06.max-800x600.png'/>\n",
    "<div style='text-align:center'> [출처 : https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab]</div>\n",
    "\n",
    "위 그림에서 보는 것처럼, <span style='color:red'>prior box를 생성하기 위해서는 먼저 기준이 되는 **feature map**을 먼저 생성합니다. </span>   \n",
    "그림에서는 8X8, 4X4의 예가 나오지만, 우리의 프로젝트에서는 아래와 같이 4가지 유형의 feature map을 생성하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "918078ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 40], [15, 20], [8, 10], [4, 5]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for m in range(4):\n",
    "    if (steps[m] != pow(2, (m + 3))):\n",
    "        print(\"steps must be [8,16,32,64]\")\n",
    "        sys.exit()\n",
    "\n",
    "assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "feature_maps = [\n",
    "    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "    for step in steps]\n",
    "\n",
    "feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a14ec",
   "metadata": {},
   "source": [
    "* `math.ceil()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb8dbc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17680"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제 feature map별로 순회를 하면서 prior box 를 생성해 보겠습니다.\n",
    "anchors = []\n",
    "num_box_fm_cell=[]\n",
    "\n",
    "for k, f in enumerate(feature_maps):\n",
    "    num_box_fm_cell.append(len(min_sizes[k]))\n",
    "    for i, j in product(range(f[0]), range(f[1])):\n",
    "        for min_size in min_sizes[k]:\n",
    "            if isinstance(min_size, int):\n",
    "                min_size = (min_size, min_size)\n",
    "            elif isinstance(min_size, tuple):\n",
    "                min_size=min_size\n",
    "            else:\n",
    "                raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "            s_kx = min_size[1] / image_sizes[1]\n",
    "            s_ky = min_size[0] / image_sizes[0]\n",
    "            cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "            cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "            anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19684ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors = np.asarray(anchors).reshape([-1, 4])\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281635e",
   "metadata": {},
   "source": [
    "* `np.asarray()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0f36f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.01666667, 0.03125   , 0.04166667],\n",
       "       [0.0125    , 0.01666667, 0.05      , 0.06666667],\n",
       "       [0.0125    , 0.01666667, 0.075     , 0.1       ],\n",
       "       ...,\n",
       "       [0.9       , 0.93333333, 0.4       , 0.53333333],\n",
       "       [0.9       , 0.93333333, 0.6       , 0.8       ],\n",
       "       [0.9       , 0.93333333, 0.8       , 1.06666667]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb814129",
   "metadata": {},
   "source": [
    "**prior_box 생성 과정** : `make_prior_box.py`   \n",
    "**prior_box 생성과정 실행** : ` cd ~/aiffel/face_detector && python make_prior_box.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee08a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior box를 생성하는 메소드\n",
    "def prior_box(cfg,image_sizes=None):\n",
    "    \"\"\"prior box\"\"\"\n",
    "    if image_sizes is None:\n",
    "        image_sizes = cfg['input_size']\n",
    "    min_sizes=cfg[\"min_sizes\"]\n",
    "    steps=cfg[\"steps\"]\n",
    "    clip=cfg[\"clip\"]\n",
    "\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    for m in range(4):\n",
    "        if (steps[m] != pow(2, (m + 3))):\n",
    "            print(\"steps must be [8,16,32,64]\")\n",
    "            sys.exit()\n",
    "\n",
    "    assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps]\n",
    "\n",
    "    anchors = []\n",
    "    num_box_fm_cell=[]\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        num_box_fm_cell.append(len(min_sizes[k]))\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                if isinstance(min_size, int):\n",
    "                    min_size = (min_size, min_size)\n",
    "                elif isinstance(min_size, tuple):\n",
    "                    min_size=min_size\n",
    "                else:\n",
    "                    raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "                s_kx = min_size[1] / image_sizes[1]\n",
    "                s_ky = min_size[0] / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "    output = np.asarray(anchors).reshape([-1, 4])\n",
    "\n",
    "    if clip:\n",
    "        output = np.clip(output, 0, 1)\n",
    "    return output,num_box_fm_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1634dd0",
   "metadata": {},
   "source": [
    "## 4. 모델 구현(2) SSD\n",
    "\n",
    "\n",
    "### SSD model 빌드하기\n",
    "\n",
    "\n",
    "#### 레이어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "138ddd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1), use_bn=True, padding=None, block_id=None):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu).\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f1dde",
   "metadata": {},
   "source": [
    "* `tf.keras.backend.get_uid()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2322e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depthwise_conv_block(inputs, pointwise_conv_filters,\n",
    "                          depth_multiplier=1, strides=(1, 1), use_bn=True, block_id=None):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "        # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "\n",
    "    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n",
    "                                        padding='same' if strides == (1, 1) else 'valid',\n",
    "                                        depth_multiplier=depth_multiplier,\n",
    "                                        strides=strides,\n",
    "                                        use_bias=False if use_bn else True,\n",
    "                                        name='conv_dw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(pointwise_conv_filters, (1, 1),\n",
    "                               padding='same',\n",
    "                               use_bias=False if use_bn else True,\n",
    "                               strides=(1, 1),\n",
    "                               name='conv_pw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34666d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _branch_block(input, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(input)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n",
    "\n",
    "    return tf.keras.layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f76f7",
   "metadata": {},
   "source": [
    "* `tf.keras.layers.Concatenate(axis=-1)()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a3dc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_head_block(inputs, filters, strides=(1, 1), block_id=None):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94d1cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_heads(x, idx, num_class, num_cell):\n",
    "    \"\"\" Compute outputs of classification and regression heads\n",
    "    Args:\n",
    "        x: the input feature map\n",
    "        idx: index of the head layer\n",
    "    Returns:\n",
    "        conf: output of the idx-th classification head\n",
    "        loc: output of the idx-th regression head\n",
    "    \"\"\"\n",
    "    conf = _create_head_block(inputs=x, filters=num_cell[idx] * num_class)\n",
    "    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n",
    "    loc = _create_head_block(inputs=x, filters=num_cell[idx] * 4)\n",
    "    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n",
    "\n",
    "    return conf, loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aeb6b6",
   "metadata": {},
   "source": [
    "#### SSD model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11cb80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어들이 준비되었습니다. 이제 본격적으로 SSD model을 생성해 보겠습니다.\n",
    "def SsdModel(cfg, num_cell, training=False, name='ssd_model'):\n",
    "    image_sizes = cfg['input_size']   if training else None\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    elif image_sizes == None:\n",
    "        image_sizes = (None, None)\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    base_channel = cfg[\"base_channel\"]\n",
    "    num_class = len(cfg['labels_list'])\n",
    "\n",
    "    x = inputs = tf.keras.layers.Input(shape=[image_sizes[0], image_sizes[1], 3], name='input_image')\n",
    "\n",
    "    x = _conv_block(x, base_channel, strides=(2, 2))  # 120*160*16\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(2, 2))  # 60*80\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(2, 2))  # 30*40\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x1 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _conv_block(x, base_channel * 8, strides=(2, 2))  # 15*20\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x2 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 8*10\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n",
    "    x3 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 4*5\n",
    "    x4 = _branch_block(x, base_channel)\n",
    "\n",
    "    extra_layers = [x1, x2, x3, x4]\n",
    "\n",
    "    confs = []\n",
    "    locs = []\n",
    "\n",
    "    head_idx = 0\n",
    "    \n",
    "    assert len(extra_layers) == len(num_cell)\n",
    "    \n",
    "    for layer in extra_layers:\n",
    "        conf, loc = _compute_heads(layer, head_idx, num_class, num_cell)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "\n",
    "        head_idx += 1\n",
    "\n",
    "    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n",
    "    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n",
    "\n",
    "    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f219c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_1 (ZeroPadding2D)      (None, None, None, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, None, None, 1 432         conv_pad_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_1 (BatchNormalization)  (None, None, None, 1 64          conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_1 (ReLU)              (None, None, None, 1 0           conv_bn_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, None, None, 3 4608        conv_relu_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_2 (BatchNormalization)  (None, None, None, 3 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_2 (ReLU)              (None, None, None, 3 0           conv_bn_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_3 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, None, None, 3 9216        conv_pad_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_3 (BatchNormalization)  (None, None, None, 3 128         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_3 (ReLU)              (None, None, None, 3 0           conv_bn_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, None, None, 3 9216        conv_relu_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_4 (BatchNormalization)  (None, None, None, 3 128         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_4 (ReLU)              (None, None, None, 3 0           conv_bn_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_5 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, None, None, 6 18432       conv_pad_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_5 (BatchNormalization)  (None, None, None, 6 256         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_5 (ReLU)              (None, None, None, 6 0           conv_bn_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, None, None, 6 36864       conv_relu_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_6 (BatchNormalization)  (None, None, None, 6 256         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_6 (ReLU)              (None, None, None, 6 0           conv_bn_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, None, None, 6 36864       conv_relu_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_7 (BatchNormalization)  (None, None, None, 6 256         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_7 (ReLU)              (None, None, None, 6 0           conv_bn_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, None, None, 6 36864       conv_relu_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_8 (BatchNormalization)  (None, None, None, 6 256         conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_8 (ReLU)              (None, None, None, 6 0           conv_bn_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_9 (ZeroPadding2D)      (None, None, None, 6 0           conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, None, None, 1 73728       conv_pad_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_9 (BatchNormalization)  (None, None, None, 1 512         conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_9 (ReLU)              (None, None, None, 1 0           conv_bn_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, None, None, 1 147456      conv_relu_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_10 (BatchNormalization) (None, None, None, 1 512         conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_10 (ReLU)             (None, None, None, 1 0           conv_bn_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, None, None, 1 147456      conv_relu_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_11 (BatchNormalization) (None, None, None, 1 512         conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_11 (ReLU)             (None, None, None, 1 0           conv_bn_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)     (None, None, None, 1 0           conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D)    (None, None, None, 1 1152        conv_pad_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormalizati (None, None, None, 1 512         conv_dw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)          (None, None, None, 1 0           conv_dw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12 (Conv2D)             (None, None, None, 2 32768       conv_dw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)          (None, None, None, 2 0           conv_pw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)          (None, None, None, 2 0           conv_dw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13 (Conv2D)             (None, None, None, 2 65536       conv_dw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)          (None, None, None, 2 0           conv_pw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_14 (ZeroPadding2D)     (None, None, None, 2 0           conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pad_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_relu (ReLU)          (None, None, None, 2 0           conv_dw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14 (Conv2D)             (None, None, None, 2 65536       conv_dw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_relu (ReLU)          (None, None, None, 2 0           conv_pw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 1 9232        conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 18448       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 36880       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 36880       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, None, None, 1 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 1 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 1 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 18464       conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 3 36896       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 3 73760       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 3 73760       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 4 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 4 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 4 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, None, None, 4 0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, None, None, 4 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, None, None, 4 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, None, None, 4 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 5196        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 8 3464        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 8 3464        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 1 5196        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 2598        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 1732        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 4 1732        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 2598        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, None, 4)      0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, None, 4)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, None, 4)      0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, None, 4)      0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, None, 2)      0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, None, 2)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, None, 2)      0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, None, 2)      0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, None, 4)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, None, 2)      0           reshape[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SsdModel(cfg, num_cell=[3, 2, 2, 3], training=False)\n",
    "print(len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a170790f",
   "metadata": {},
   "source": [
    "* **SSD model 생성 과정** : `tf_build_ssd_model.py` \n",
    "* **SSD model 생성 과정 실행** : ` cd ~/aiffel/face_detector && python tf_build_ssd_model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720b263",
   "metadata": {},
   "source": [
    "## 5. 모델 학습(1) Augmentation, Prior 적용\n",
    "\n",
    "\n",
    "### augmentation\n",
    "\n",
    "* augmentation을 위해 `tf.data.TFRecordDataset.map()` 내에서 호출할 메소드들\n",
    "  - _crop\n",
    "  - _pad_to_square\n",
    "  - _resize\n",
    "  - _flip\n",
    "  - _distort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15352ab0",
   "metadata": {},
   "source": [
    "#### crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55f68d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop(img, labels, max_loop=250):\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    def matrix_iof(a, b):\n",
    "        \"\"\"\n",
    "        return iof of a and b, numpy version for data augenmentation\n",
    "        \"\"\"\n",
    "        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n",
    "        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n",
    "            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n",
    "        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n",
    "\n",
    "    def crop_loop_body(i, img, labels):\n",
    "        valid_crop = tf.constant(1, tf.int32)\n",
    "\n",
    "        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n",
    "        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n",
    "        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n",
    "        h = w = tf.cast(scale * short_side, tf.int32)\n",
    "        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n",
    "        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n",
    "        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n",
    "        roi = tf.cast(roi, tf.float32)\n",
    "\n",
    "\n",
    "        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n",
    "        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n",
    "        mask_a = tf.reduce_all(\n",
    "            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n",
    "            axis=1)\n",
    "        labels_t = tf.boolean_mask(labels, mask_a)\n",
    "        valid_crop = tf.cond(tf.reduce_any(mask_a),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n",
    "        h_offset = tf.cast(h_offset, tf.float32)\n",
    "        w_offset = tf.cast(w_offset, tf.float32)\n",
    "        labels_t = tf.stack(\n",
    "            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n",
    "             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n",
    "             labels_t[:, 4]], axis=1)\n",
    "\n",
    "        return tf.cond(valid_crop == 1,\n",
    "                       lambda: (max_loop, img_t, labels_t),\n",
    "                       lambda: (i + 1, img, labels))\n",
    "\n",
    "    _, img, labels = tf.while_loop(\n",
    "        lambda i, img, labels: tf.less(i, max_loop),\n",
    "        crop_loop_body,\n",
    "        [tf.constant(-1), img, labels],\n",
    "        shape_invariants=[tf.TensorShape([]),\n",
    "                          tf.TensorShape([None, None, 3]),\n",
    "                          tf.TensorShape([None, 5])])\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4800566",
   "metadata": {},
   "source": [
    "* `tf.math.reduce_prod()` :\n",
    "* `tf.cond()` :\n",
    "* `tf.math.reduce_any()` :\n",
    "* `tf.reduce_all()` :\n",
    "* `tf.math.logical_and()` :\n",
    "* `tf.boolean_mask()` :\n",
    "* `tf.while_loop()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dba1a",
   "metadata": {},
   "source": [
    "#### pad_to_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1de0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_to_square(img):\n",
    "    height = tf.shape(img)[0]\n",
    "    width = tf.shape(img)[1]\n",
    "\n",
    "    def pad_h():\n",
    "        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_h], axis=0)\n",
    "\n",
    "    def pad_w():\n",
    "        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_w], axis=1)\n",
    "\n",
    "    img = tf.case([(tf.greater(height, width), pad_w),\n",
    "                   (tf.less(height, width), pad_h)], default=lambda: img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784cb72",
   "metadata": {},
   "source": [
    "* `tf.case()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92d7f9",
   "metadata": {},
   "source": [
    "#### resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bf7b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize(img, labels, img_dim):\n",
    "    ''' # resize and boxes coordinate to percent'''\n",
    "    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n",
    "                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n",
    "    locs = tf.clip_by_value(locs, 0, 1.0)\n",
    "    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n",
    "\n",
    "    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n",
    "    if isinstance(img_dim, int):\n",
    "        img_dim = (img_dim, img_dim)\n",
    "    elif isinstance(img_dim,tuple):\n",
    "        img_dim = img_dim\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    def resize(method):\n",
    "        def _resize():\n",
    "            #　size h,w\n",
    "            return tf.image.resize(img, [img_dim[0], img_dim[1]], method=method, antialias=True)\n",
    "        return _resize\n",
    "\n",
    "    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n",
    "                   (tf.equal(resize_case, 1), resize('area')),\n",
    "                   (tf.equal(resize_case, 2), resize('nearest')),\n",
    "                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n",
    "                  default=resize('bilinear'))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049334c",
   "metadata": {},
   "source": [
    "* `tf.clip_by_value()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec929c5a",
   "metadata": {},
   "source": [
    "#### flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3aaf463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flip(img, labels):\n",
    "    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n",
    "\n",
    "    def flip_func():\n",
    "        flip_img = tf.image.flip_left_right(img)\n",
    "        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n",
    "                                1 - labels[:, 0],  labels[:, 3],\n",
    "                                labels[:, 4]], axis=1)\n",
    "\n",
    "        return flip_img, flip_labels\n",
    "\n",
    "    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cb4c7",
   "metadata": {},
   "source": [
    "#### distort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66b44c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distort(img):\n",
    "    img = tf.image.random_brightness(img, 0.4)\n",
    "    img = tf.image.random_contrast(img, 0.5, 1.5)\n",
    "    img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "    img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f22f9eb",
   "metadata": {},
   "source": [
    "### Prior box 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96a77675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior box와 bounding box 사이의 IoU, 다른 말로 jaccard index를 측정하기 위한 함수\n",
    "def _intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2]:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    \n",
    "    max_xy = tf.minimum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n",
    "    \n",
    "    min_xy = tf.maximum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n",
    "    \n",
    "    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n",
    "    \n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af2a2c",
   "metadata": {},
   "source": [
    "* `tf.shape()` :\n",
    "* `tf.broadcast_to()` :\n",
    "* `tf.expand_dims()` :\n",
    "* `tf.clip_by_value()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "062616ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaccard index를 계산하는 함수\n",
    "def _jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = _intersect(box_a, box_b)\n",
    "    \n",
    "    area_a = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    \n",
    "    area_b = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    \n",
    "    union = area_a + area_b - inter\n",
    "    \n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f880cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox의 scale을 동일하게 보정\n",
    "def _encode_bbox(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth\n",
    "    boxes we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n",
    "\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = tf.math.log(g_wh) / variances[1]\n",
    "\n",
    "    # return target for smooth_l1_loss\n",
    "    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d693c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tf(labels, priors, match_thresh, variances=None):\n",
    "    \"\"\"tensorflow encoding\"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    priors = tf.cast(priors, tf.float32)\n",
    "    bbox = labels[:, :4]\n",
    "    conf = labels[:, -1]\n",
    "\n",
    "    # jaccard index\n",
    "    overlaps = _jaccard(bbox, priors)\n",
    "    best_prior_overlap = tf.reduce_max(overlaps, 1)\n",
    "    best_prior_idx = tf.argmax(overlaps, 1, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.reduce_max(overlaps, 0)\n",
    "    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.tensor_scatter_nd_update(\n",
    "        best_truth_overlap, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.ones_like(best_prior_idx, tf.float32) * 2.)\n",
    "    \n",
    "    best_truth_idx = tf.tensor_scatter_nd_update(\n",
    "        best_truth_idx, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.range(tf.size(best_prior_idx), dtype=tf.int32))\n",
    "\n",
    "    # Scale Ground-Truth Boxes\n",
    "    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\n",
    "    loc_t = _encode_bbox(matches_bbox, priors, variances)\n",
    "    conf_t = tf.gather(conf, best_truth_idx)  # [num_priors]\n",
    "    conf_t = tf.where(tf.less(best_truth_overlap, match_thresh), tf.zeros_like(conf_t), conf_t)\n",
    "\n",
    "    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1c98c",
   "metadata": {},
   "source": [
    "* `tf.reduce_max()` :\n",
    "* `tf.argmax()` :\n",
    "* `tf.tensor_scatter_nd_update()` :\n",
    "* `tf.gather()` :\n",
    "* `tf.where()` :\n",
    "* `tf.less()` :\n",
    "* `tf.zeros_like()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899bc8f4",
   "metadata": {},
   "source": [
    "### load_dataset\n",
    "\n",
    "위에서 구현한 두가지 메소드를 이전 스텝에서 생성한 tfrecord 데이터셋에 적용하여 SSD 학습을 위한 데이터셋을 생성하는 최종 메소드인 **load_dataset** 을 구현합니다.\n",
    "\n",
    "* `_transform_data` : `augmentation`과 `prior box label`을 적용하여 기존의 dataset을 변환하는 메소드\n",
    "* `_parse_tfrecord` : tfrecord 에 `_transform_data`를 적용하는 함수 클로저 생성\n",
    "* `load_tfrecord_dataset` : `tf.data.TFRecordDataset.map()`에 `_parse_tfrecord`을 적용하는 실제 데이터셋 변환 메인 메소드\n",
    "* `load_dataset` : `load_tfrecord_dataset`을 통해 `train, validation` 데이터셋을 생성하는 최종 메소드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0baa750",
   "metadata": {},
   "source": [
    "#### transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b4e2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_data(img_dim, using_crop,using_flip, using_distort, using_encoding,using_normalizing, priors,\n",
    "                    match_thresh,  variances):\n",
    "    def transform_data(img, labels):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        if using_crop:\n",
    "        # randomly crop\n",
    "            img, labels = _crop(img, labels)\n",
    "\n",
    "            # padding to square\n",
    "            img = _pad_to_square(img)\n",
    "\n",
    "        # resize and boxes coordinate to percent\n",
    "        img, labels = _resize(img, labels, img_dim)\n",
    "\n",
    "        # randomly left-right flip\n",
    "        if using_flip:\n",
    "            img, labels = _flip(img, labels)\n",
    "\n",
    "        # distort\n",
    "        if using_distort:\n",
    "            img = _distort(img)\n",
    "\n",
    "        # encode labels to feature targets\n",
    "        if using_encoding:\n",
    "            labels = encode_tf(labels=labels, priors=priors, match_thresh=match_thresh, variances=variances)\n",
    "        if using_normalizing:\n",
    "            img=(img/255.0-0.5)/1.0\n",
    "\n",
    "        return img, labels\n",
    "    return transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879d4ea",
   "metadata": {},
   "source": [
    "#### parse_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19a35405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfrecord(img_dim,using_crop, using_flip, using_distort,\n",
    "                    using_encoding, using_normalizing,priors, match_thresh,  variances):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'difficult':tf.io.VarLenFeature(tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "           }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "\n",
    "        width = tf.cast(parsed_example['width'], tf.float32)\n",
    "        height = tf.cast(parsed_example['height'], tf.float32)\n",
    "\n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        labels = tf.stack(\n",
    "            [tf.sparse.to_dense(parsed_example['x_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['y_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['x_maxes']),\n",
    "             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n",
    "\n",
    "        img, labels = _transform_data(\n",
    "            img_dim, using_crop,using_flip, using_distort, using_encoding, using_normalizing,priors,\n",
    "            match_thresh,  variances)(img, labels)\n",
    "\n",
    "        return img, labels\n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68f776",
   "metadata": {},
   "source": [
    "* `tf.io.FixedLenFeature()` :\n",
    "* `tf.io.parse_single_example()` :\n",
    "* `tf.image.decode_jpeg()` :\n",
    "* `tf.sparse.to_dense()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996975a",
   "metadata": {},
   "source": [
    "#### load_tfrecord_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b56274eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\n",
    "                          using_crop=True,using_flip=True, using_distort=True,\n",
    "                          using_encoding=True, using_normalizing=True,\n",
    "                          priors=None, match_thresh=0.45,variances=None,\n",
    "                          shuffle=True, repeat=True,buffer_size=10240):\n",
    "\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    if not using_encoding:\n",
    "        assert batch_size == 1\n",
    "    else:\n",
    "        assert priors is not None\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache()\n",
    "    if repeat:\n",
    "        raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(img_dim, using_crop, using_flip, using_distort,\n",
    "                        using_encoding, using_normalizing,priors, match_thresh,  variances),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1b298",
   "metadata": {},
   "source": [
    "* `tf.data.TFRecordDataset()` :\n",
    "* `.cache()` :\n",
    "* `.repeat()` :\n",
    "* `.map()` :\n",
    "* `.batch()` :\n",
    "* `.prefetch()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bbe9f",
   "metadata": {},
   "source": [
    "#### load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1887a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(cfg, priors, shuffle=True, buffer_size=10240,train=True):\n",
    "    \"\"\"load dataset\"\"\"\n",
    "    global dataset\n",
    "    if train:\n",
    "        logging.info(\"load train dataset from {}\".format(cfg['dataset_path']))\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['dataset_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=cfg['using_crop'],\n",
    "            using_flip=cfg['using_flip'],\n",
    "            using_distort=cfg['using_distort'],\n",
    "            using_encoding=True,\n",
    "            using_normalizing=cfg['using_normalizing'],\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=True,\n",
    "            buffer_size=buffer_size)\n",
    "    else:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['val_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=False,\n",
    "            using_flip=False,\n",
    "            using_distort=False,\n",
    "            using_encoding=True,\n",
    "            using_normalizing=cfg['using_normalizing'],   # ????? cfg['using_normalizing'],  True\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=False,\n",
    "            buffer_size=buffer_size)\n",
    "        logging.info(\"load validation dataset from {}\".format(cfg['val_path']))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795e4c7",
   "metadata": {},
   "source": [
    "* `logging.info()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795f3b1",
   "metadata": {},
   "source": [
    "이번 스텝에 소개한 load_dataset 메소드 구현체를 `tf_dataloader.py`에 정리해 두었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1f235",
   "metadata": {},
   "source": [
    "## 6. 모델 학습(2) Train\n",
    "\n",
    "\n",
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66f276",
   "metadata": {},
   "source": [
    "* [참고: SSD(Code-Model,Train & Test)](https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(17)/)\n",
    "* [참고: SSD (Single Shot MultiBox Detector) - Tensorflow 2.0](https://github.com/ChunML/ssd-tf2)\n",
    "* [참고: Pytorch Learning Rate Scheduler (러닝 레이트 스케쥴러) 정리](https://gaussian37.github.io/dl-pytorch-lr_scheduler/)\n",
    "* [참고: Optimization](https://huggingface.co/transformers/main_classes/optimizer_schedules.html)\n",
    "* [참고: Learning Rate Scheduling](https://d2l.ai/chapter_optimization/lr-scheduler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f39aa623",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseConstantWarmUpDecay(\n",
    "        tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule wiht warm up schedule.\n",
    "    Modified from tf.keras.optimizers.schedules.PiecewiseConstantDecay\"\"\"\n",
    "\n",
    "    def __init__(self, boundaries, values, warmup_steps, min_lr,\n",
    "                 name=None):\n",
    "        super(PiecewiseConstantWarmUpDecay, self).__init__()\n",
    "\n",
    "        if len(boundaries) != len(values) - 1:\n",
    "            raise ValueError(\n",
    "                    \"The length of boundaries should be 1 less than the\"\n",
    "                    \"length of values\")\n",
    "\n",
    "        self.boundaries = boundaries\n",
    "        self.values = values\n",
    "        self.name = name\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n",
    "            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n",
    "            pred_fn_pairs = []\n",
    "            warmup_steps = self.warmup_steps\n",
    "            boundaries = self.boundaries\n",
    "            values = self.values\n",
    "            min_lr = self.min_lr\n",
    "\n",
    "            pred_fn_pairs.append(\n",
    "                (step <= warmup_steps,\n",
    "                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n",
    "            pred_fn_pairs.append(\n",
    "                (tf.logical_and(step <= boundaries[0],\n",
    "                                step > warmup_steps),\n",
    "                 lambda: tf.constant(values[0])))\n",
    "            pred_fn_pairs.append(\n",
    "                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n",
    "\n",
    "            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n",
    "                                    values[1:-1]):\n",
    "                # Need to bind v here; can do this with lambda v=v: ...\n",
    "                pred = (step > low) & (step <= high)\n",
    "                pred_fn_pairs.append((pred, lambda: tf.constant(v)))\n",
    "\n",
    "            # The default isn't needed here because our conditions are mutually\n",
    "            # exclusive and exhaustive, but tf.case requires it.\n",
    "            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n",
    "                           exclusive=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"boundaries\": self.boundaries,\n",
    "                \"values\": self.values,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "                \"min_lr\": self.min_lr,\n",
    "                \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99452bd7",
   "metadata": {},
   "source": [
    "* `tf.keras.optimizers.schedules.LearningRateSchedule` :\n",
    "* `tf.name_scope()` :\n",
    "* `tf.convert_to_tensor()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22376957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n",
    "                      warmup_steps=0., min_lr=0.,\n",
    "                      name='MultiStepWarmUpLR'):\n",
    "    \"\"\"Multi-steps warm up learning rate scheduler.\"\"\"\n",
    "    assert warmup_steps <= lr_steps[0]\n",
    "    assert min_lr <= initial_learning_rate\n",
    "    \n",
    "    lr_steps_value = [initial_learning_rate]\n",
    "    \n",
    "    for _ in range(len(lr_steps)):\n",
    "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
    "        \n",
    "    return PiecewiseConstantWarmUpDecay(\n",
    "        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7e43b",
   "metadata": {},
   "source": [
    "* 가정 설정문(assert) :  assert는 뒤의 조건이 True가 아니면 AssertError를 발생한다. \n",
    "  - assert 조건식, '에러메시지'\n",
    "    - '에러메시지'는 생략할 수 있다.\n",
    "  - [참고1](https://wikidocs.net/21050)\n",
    "  - [참고2](https://wikidocs.net/84431)\n",
    "  - [참고3](https://dojang.io/mod/page/view.php?id=2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f4e74",
   "metadata": {},
   "source": [
    "### Hard negative mining\n",
    "\n",
    "Object Detection 모델 학습시 자주 사용되는 **Hard negative mining**이라는 기법이 있습니다.   \n",
    "<span style='color:red'>학습과정에서 label은 negative인데 confidence가 높게 나오는 샘플을 재학습하면 positive와 negative의 모호한 경계선상에 분포한 false negative 오류에 강해진다는 장점이 있습니다. </span>\n",
    "\n",
    "<span style='color:blue'> 실제로 confidence가 높은 샘플을 모아 training을 다시 수행하기보다는, 그런 샘플들에 대한 loss만 따로 모아 계산해주는 방식으로 반영할 수 있습니다.</span>\n",
    "\n",
    "아래 구현된 hard_negative_mining 메소드와, 이 메소드를 통해 얻은 샘플을 통해 얻은 localization loss를 기존의 classification loss에 추가로 반영하는 **MultiBoxLoss** 계산 메소드를 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7c46aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_mining(loss, class_truth, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        class_truth: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        class_loss: classification loss\n",
    "        loc_loss: regression loss\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # class_truth: B x N\n",
    "    pos_idx = class_truth > 0\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
    "    rank = tf.argsort(rank, axis=1)\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
    "\n",
    "    return pos_idx, neg_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94e88a",
   "metadata": {},
   "source": [
    "* `tf.reduce_sum()` :\n",
    "* `tf.argsort()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dca1dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiBoxLoss(num_class=3, neg_pos_ratio=3.0):\n",
    "    def multi_loss(y_true, y_pred):\n",
    "        \"\"\" Compute losses for SSD\n",
    "               regression loss: smooth L1\n",
    "               classification loss: cross entropy\n",
    "           Args:\n",
    "               y_true: [B,N,5]\n",
    "               y_pred: [B,N,num_class]\n",
    "               class_pred: outputs of classification heads (B,N, num_classes)\n",
    "               loc_pred: outputs of regression heads (B,N, 4)\n",
    "               class_truth: classification targets (B,N)\n",
    "               loc_truth: regression targets (B,N, 4)\n",
    "           Returns:\n",
    "               class_loss: classification loss\n",
    "               loc_loss: regression loss\n",
    "       \"\"\"\n",
    "        num_batch = tf.shape(y_true)[0]\n",
    "        num_prior = tf.shape(y_true)[1]\n",
    "        loc_pred, class_pred = y_pred[..., :4], y_pred[..., 4:]\n",
    "        loc_truth, class_truth = y_true[..., :4], tf.squeeze(y_true[..., 4:])\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses without reduction\n",
    "        temp_loss = cross_entropy(class_truth, class_pred)\n",
    "        # 2. hard negative mining\n",
    "        pos_idx, neg_idx = hard_negative_mining(temp_loss, class_truth, neg_pos_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n",
    "\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        loss_class = cross_entropy(\n",
    "            class_truth[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            class_pred[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # localization loss only consist of positive examples (smooth L1)\n",
    "        loss_loc = smooth_l1_loss(loc_truth[pos_idx],loc_pred[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "\n",
    "        loss_class = loss_class / num_pos\n",
    "        loss_loc = loss_loc / num_pos\n",
    "        return loss_loc, loss_class\n",
    "\n",
    "    return multi_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7235d",
   "metadata": {},
   "source": [
    "* `tf.keras.losses.SparseCategoricalCrossentropy()` :\n",
    "* `tf.keras.losses.Huber()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc4098",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "이제 본격적으로 모델 학습을 진행하겠습니다. 배치사이즈, epoch 수 등 학습에 대한 기본설정은 **cfg dict 내용**을 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebbd14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global load_t1\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "\n",
    "weights_dir = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "\n",
    "logging.info(\"Load configuration...\")\n",
    "label_classes = cfg['labels_list']\n",
    "logging.info(f\"Total image sample:{cfg['dataset_len']},Total classes number:\"\n",
    "             f\"{len(label_classes)},classes list:{label_classes}\")\n",
    "\n",
    "logging.info(\"Compute prior boxes...\")\n",
    "priors, num_cell = prior_box(cfg)\n",
    "logging.info(f\"Prior boxes number:{len(priors)},default anchor box number per feature map cell:{num_cell}\") # 4420, [3, 2, 2, 3]\n",
    "\n",
    "logging.info(\"Loading dataset...\")\n",
    "train_dataset = load_dataset(cfg, priors, shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9fd287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 240, 320, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_43 (ZeroPadding2D)     (None, 242, 322, 3)  0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_43 (Conv2D)                (None, 120, 160, 16) 432         conv_pad_43[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_43 (BatchNormalization) (None, 120, 160, 16) 64          conv_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_43 (ReLU)             (None, 120, 160, 16) 0           conv_bn_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_44 (Conv2D)                (None, 120, 160, 32) 4608        conv_relu_43[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_44 (BatchNormalization) (None, 120, 160, 32) 128         conv_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_44 (ReLU)             (None, 120, 160, 32) 0           conv_bn_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_45 (ZeroPadding2D)     (None, 122, 162, 32) 0           conv_relu_44[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_45 (Conv2D)                (None, 60, 80, 32)   9216        conv_pad_45[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_45 (BatchNormalization) (None, 60, 80, 32)   128         conv_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_45 (ReLU)             (None, 60, 80, 32)   0           conv_bn_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_46 (Conv2D)                (None, 60, 80, 32)   9216        conv_relu_45[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_46 (BatchNormalization) (None, 60, 80, 32)   128         conv_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_46 (ReLU)             (None, 60, 80, 32)   0           conv_bn_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_47 (ZeroPadding2D)     (None, 62, 82, 32)   0           conv_relu_46[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_47 (Conv2D)                (None, 30, 40, 64)   18432       conv_pad_47[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_47 (BatchNormalization) (None, 30, 40, 64)   256         conv_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_47 (ReLU)             (None, 30, 40, 64)   0           conv_bn_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_48 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_47[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_48 (BatchNormalization) (None, 30, 40, 64)   256         conv_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_48 (ReLU)             (None, 30, 40, 64)   0           conv_bn_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_49 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_48[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_49 (BatchNormalization) (None, 30, 40, 64)   256         conv_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_49 (ReLU)             (None, 30, 40, 64)   0           conv_bn_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_50 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_49[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_50 (BatchNormalization) (None, 30, 40, 64)   256         conv_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_50 (ReLU)             (None, 30, 40, 64)   0           conv_bn_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_51 (ZeroPadding2D)     (None, 32, 42, 64)   0           conv_relu_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_51 (Conv2D)                (None, 15, 20, 128)  73728       conv_pad_51[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_51 (BatchNormalization) (None, 15, 20, 128)  512         conv_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_51 (ReLU)             (None, 15, 20, 128)  0           conv_bn_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_52 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_51[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_52 (BatchNormalization) (None, 15, 20, 128)  512         conv_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_52 (ReLU)             (None, 15, 20, 128)  0           conv_bn_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_53 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_53 (BatchNormalization) (None, 15, 20, 128)  512         conv_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_53 (ReLU)             (None, 15, 20, 128)  0           conv_bn_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_54 (ZeroPadding2D)     (None, 17, 22, 128)  0           conv_relu_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_54 (DepthwiseConv2D)    (None, 8, 10, 128)   1152        conv_pad_54[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_54_bn (BatchNormalizati (None, 8, 10, 128)   512         conv_dw_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_54_relu (ReLU)          (None, 8, 10, 128)   0           conv_dw_54_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_54 (Conv2D)             (None, 8, 10, 256)   32768       conv_dw_54_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_54_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_54_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_54_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_55 (DepthwiseConv2D)    (None, 8, 10, 256)   2304        conv_pw_54_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_55_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_dw_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_55_relu (ReLU)          (None, 8, 10, 256)   0           conv_dw_55_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_55 (Conv2D)             (None, 8, 10, 256)   65536       conv_dw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_55_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_55_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_55_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_56 (ZeroPadding2D)     (None, 10, 12, 256)  0           conv_pw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_56 (DepthwiseConv2D)    (None, 4, 5, 256)    2304        conv_pad_56[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_56_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_dw_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_56_relu (ReLU)          (None, 4, 5, 256)    0           conv_dw_56_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_56 (Conv2D)             (None, 4, 5, 256)    65536       conv_dw_56_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_56_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_pw_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_56_relu (ReLU)          (None, 4, 5, 256)    0           conv_pw_56_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 30, 40, 16)   9232        conv_relu_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 15, 20, 16)   18448       conv_relu_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 8, 10, 16)    36880       conv_pw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 4, 5, 16)     36880       conv_pw_56_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 30, 40, 16)   0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 15, 20, 16)   0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 8, 10, 16)    0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 4, 5, 16)     0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 30, 40, 16)   2320        leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 30, 40, 32)   18464       conv_relu_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 15, 20, 16)   2320        leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 15, 20, 32)   36896       conv_relu_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 8, 10, 16)    2320        leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 10, 32)    73760       conv_pw_55_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 4, 5, 16)     2320        leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 4, 5, 32)     73760       conv_pw_56_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 30, 40, 48)   0           conv2d_61[0][0]                  \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 15, 20, 48)   0           conv2d_64[0][0]                  \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 10, 48)    0           conv2d_67[0][0]                  \n",
      "                                                                 conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 4, 5, 48)     0           conv2d_70[0][0]                  \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 30, 40, 48)   0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 15, 20, 48)   0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 8, 10, 48)    0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 4, 5, 48)     0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 30, 40, 12)   5196        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 15, 20, 8)    3464        re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 10, 8)     3464        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 4, 5, 12)     5196        re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 30, 40, 6)    2598        re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 15, 20, 4)    1732        re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 10, 4)     1732        re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 4, 5, 6)      2598        re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, None, 4)      0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, None, 4)      0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, None, 4)      0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_31 (Reshape)            (None, None, 4)      0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, None, 2)      0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, None, 2)      0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, None, 2)      0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, None, 2)      0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, None, 4)      0           reshape_25[0][0]                 \n",
      "                                                                 reshape_27[0][0]                 \n",
      "                                                                 reshape_29[0][0]                 \n",
      "                                                                 reshape_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, None, 2)      0           reshape_24[0][0]                 \n",
      "                                                                 reshape_26[0][0]                 \n",
      "                                                                 reshape_28[0][0]                 \n",
      "                                                                 reshape_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Create Model...\")\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=True)\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file=os.path.join(os.getcwd(), 'model.png'),\n",
    "                              show_shapes=True, show_layer_names=True)\n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "    logging.info(\"Create network failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9eda2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['resume']:\n",
    "    # Training from latest weights\n",
    "    paths = [os.path.join(weights_dir, path)\n",
    "             for path in os.listdir(weights_dir)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    init_epoch = int(os.path.splitext(latest)[0][-3:])\n",
    "\n",
    "else:\n",
    "    init_epoch = -1\n",
    "\n",
    "steps_per_epoch = cfg['dataset_len'] // cfg['batch_size']\n",
    "logging.info(f\"steps_per_epoch:{steps_per_epoch}\")\n",
    "\n",
    "learning_rate = MultiStepWarmUpLR(\n",
    "    initial_learning_rate=cfg['init_lr'],\n",
    "    lr_steps=[e * steps_per_epoch for e in cfg['lr_decay_epoch']],\n",
    "    lr_rate=cfg['lr_rate'],\n",
    "    warmup_steps=cfg['warmup_epoch'] * steps_per_epoch,\n",
    "    min_lr=cfg['min_lr'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=cfg['momentum'], nesterov=True)\n",
    "multi_loss = MultiBoxLoss(num_class=len(label_classes), neg_pos_ratio=3)\n",
    "train_log_dir = 'logs/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23ee1f",
   "metadata": {},
   "source": [
    "* `os.path.getmtime` :\n",
    "* `model.load_weights()` :\n",
    "* `os.path.splitext()` :\n",
    "* `tf.summary.create_file_writer()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb508398",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        losses = {}\n",
    "        losses['reg'] = tf.reduce_sum(model.losses)  #unused. Init for redefine network\n",
    "        losses['loc'], losses['class'] = multi_loss(labels, predictions)\n",
    "        total_loss = tf.add_n([l for l in losses.values()])\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d9725",
   "metadata": {},
   "source": [
    "* `tape.gradient()` :\n",
    "* `optimizer.apply_gradients()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e797c",
   "metadata": {},
   "source": [
    "아래에서 본격적으로 train을 시작합니다. 1Epoch당 1분 가량 소요되며, 총 100Epoch 학습이 진행됩니다. 소요시간에 유의해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dc76a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 | Batch 402/402 | Batch time 0.091 || Loss: 15.436087 | loc loss:13.192349 | class loss:2.243737 \n",
      "Epoch: 1/100  | Epoch time 54.294 || Average Loss: inf\n",
      "Epoch: 2/100 | Batch 402/402 | Batch time 0.086 || Loss: 7.178912 | loc loss:4.887761 | class loss:2.291152   \n",
      "Epoch: 2/100  | Epoch time 49.160 || Average Loss: inf\n",
      "Epoch: 3/100 | Batch 402/402 | Batch time 0.111 || Loss: 5.337385 | loc loss:3.257466 | class loss:2.079919  \n",
      "Epoch: 3/100  | Epoch time 49.240 || Average Loss: inf\n",
      "Epoch: 4/100 | Batch 402/402 | Batch time 0.111 || Loss: 5.174000 | loc loss:3.119871 | class loss:2.054128  \n",
      "Epoch: 4/100  | Epoch time 48.869 || Average Loss: inf\n",
      "Epoch: 5/100 | Batch 402/402 | Batch time 0.108 || Loss: 4.817545 | loc loss:3.028877 | class loss:1.788668  \n",
      "Epoch: 5/100  | Epoch time 50.383 || Average Loss: inf\n",
      "Epoch: 6/100 | Batch 402/402 | Batch time 0.083 || Loss: 7.386495 | loc loss:5.477237 | class loss:1.909258  \n",
      "Epoch: 6/100  | Epoch time 48.554 || Average Loss: inf\n",
      "Epoch: 7/100 | Batch 402/402 | Batch time 0.128 || Loss: 7.044664 | loc loss:5.117079 | class loss:1.927585  \n",
      "Epoch: 7/100  | Epoch time 49.992 || Average Loss: inf\n",
      "Epoch: 8/100 | Batch 402/402 | Batch time 0.092 || Loss: 6.117234 | loc loss:4.209730 | class loss:1.907504 \n",
      "Epoch: 8/100  | Epoch time 48.764 || Average Loss: inf\n",
      "Epoch: 9/100 | Batch 402/402 | Batch time 0.096 || Loss: 5.595008 | loc loss:3.752402 | class loss:1.842606 \n",
      "Epoch: 9/100  | Epoch time 48.841 || Average Loss: inf\n",
      "Epoch: 10/100 | Batch 402/402 | Batch time 0.098 || Loss: 5.800219 | loc loss:3.998332 | class loss:1.801887 7 \n",
      "Epoch: 10/100  | Epoch time 49.759 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_010.h5<<<<<<<<<<\n",
      "Epoch: 11/100 | Batch 402/402 | Batch time 0.129 || Loss: 5.593855 | loc loss:3.715664 | class loss:1.878191 \n",
      "Epoch: 11/100  | Epoch time 47.808 || Average Loss: inf\n",
      "Epoch: 12/100 | Batch 402/402 | Batch time 0.153 || Loss: 6.761444 | loc loss:4.723257 | class loss:2.038187  \n",
      "Epoch: 12/100  | Epoch time 50.058 || Average Loss: inf\n",
      "Epoch: 13/100 | Batch 402/402 | Batch time 0.116 || Loss: 7.013397 | loc loss:5.063138 | class loss:1.950259 \n",
      "Epoch: 13/100  | Epoch time 49.748 || Average Loss: inf\n",
      "Epoch: 14/100 | Batch 402/402 | Batch time 0.112 || Loss: 7.201136 | loc loss:5.343570 | class loss:1.857566 \n",
      "Epoch: 14/100  | Epoch time 49.688 || Average Loss: inf\n",
      "Epoch: 15/100 | Batch 402/402 | Batch time 0.114 || Loss: 4.351018 | loc loss:2.325129 | class loss:2.025889 \n",
      "Epoch: 15/100  | Epoch time 49.486 || Average Loss: inf\n",
      "Epoch: 16/100 | Batch 402/402 | Batch time 0.120 || Loss: 6.935164 | loc loss:4.974706 | class loss:1.960458  \n",
      "Epoch: 16/100  | Epoch time 49.624 || Average Loss: inf\n",
      "Epoch: 17/100 | Batch 402/402 | Batch time 0.122 || Loss: 6.367636 | loc loss:4.431823 | class loss:1.935812  \n",
      "Epoch: 17/100  | Epoch time 49.268 || Average Loss: inf\n",
      "Epoch: 18/100 | Batch 402/402 | Batch time 0.096 || Loss: 6.519467 | loc loss:4.412614 | class loss:2.106853 \n",
      "Epoch: 18/100  | Epoch time 49.837 || Average Loss: inf\n",
      "Epoch: 19/100 | Batch 402/402 | Batch time 0.122 || Loss: 4.805065 | loc loss:3.017929 | class loss:1.787136 \n",
      "Epoch: 19/100  | Epoch time 50.057 || Average Loss: inf\n",
      "Epoch: 20/100 | Batch 402/402 | Batch time 0.133 || Loss: 7.125689 | loc loss:5.054121 | class loss:2.071567 \n",
      "Epoch: 20/100  | Epoch time 48.352 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_020.h5<<<<<<<<<<\n",
      "Epoch: 21/100 | Batch 402/402 | Batch time 0.105 || Loss: 8.250031 | loc loss:6.443999 | class loss:1.806033 \n",
      "Epoch: 21/100  | Epoch time 48.012 || Average Loss: inf\n",
      "Epoch: 22/100 | Batch 402/402 | Batch time 0.096 || Loss: 6.487029 | loc loss:4.707383 | class loss:1.779646  \n",
      "Epoch: 22/100  | Epoch time 47.989 || Average Loss: inf\n",
      "Epoch: 23/100 | Batch 402/402 | Batch time 0.077 || Loss: 5.130416 | loc loss:3.136025 | class loss:1.994391 \n",
      "Epoch: 23/100  | Epoch time 49.552 || Average Loss: inf\n",
      "Epoch: 24/100 | Batch 402/402 | Batch time 0.104 || Loss: 6.250828 | loc loss:4.348918 | class loss:1.901910 \n",
      "Epoch: 24/100  | Epoch time 49.551 || Average Loss: inf\n",
      "Epoch: 25/100 | Batch 402/402 | Batch time 0.090 || Loss: 6.286024 | loc loss:4.576295 | class loss:1.709729 \n",
      "Epoch: 25/100  | Epoch time 48.604 || Average Loss: inf\n",
      "Epoch: 26/100 | Batch 402/402 | Batch time 0.168 || Loss: 7.492663 | loc loss:5.585618 | class loss:1.907045  \n",
      "Epoch: 26/100  | Epoch time 48.999 || Average Loss: inf\n",
      "Epoch: 27/100 | Batch 402/402 | Batch time 0.096 || Loss: 5.563128 | loc loss:3.706232 | class loss:1.856896  \n",
      "Epoch: 27/100  | Epoch time 48.516 || Average Loss: inf\n",
      "Epoch: 28/100 | Batch 402/402 | Batch time 0.086 || Loss: 5.157140 | loc loss:3.225635 | class loss:1.931505  \n",
      "Epoch: 28/100  | Epoch time 49.249 || Average Loss: inf\n",
      "Epoch: 29/100 | Batch 402/402 | Batch time 0.124 || Loss: 7.412015 | loc loss:5.390524 | class loss:2.021490  \n",
      "Epoch: 29/100  | Epoch time 49.524 || Average Loss: inf\n",
      "Epoch: 30/100 | Batch 402/402 | Batch time 0.118 || Loss: 5.436100 | loc loss:3.740740 | class loss:1.695360  \n",
      "Epoch: 30/100  | Epoch time 48.187 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_030.h5<<<<<<<<<<\n",
      "Epoch: 31/100 | Batch 402/402 | Batch time 0.118 || Loss: 6.699335 | loc loss:4.813157 | class loss:1.886179  \n",
      "Epoch: 31/100  | Epoch time 48.960 || Average Loss: inf\n",
      "Epoch: 32/100 | Batch 402/402 | Batch time 0.104 || Loss: 5.375223 | loc loss:3.486928 | class loss:1.888295 \n",
      "Epoch: 32/100  | Epoch time 49.322 || Average Loss: inf\n",
      "Epoch: 33/100 | Batch 402/402 | Batch time 0.119 || Loss: 7.520140 | loc loss:5.495461 | class loss:2.024679  \n",
      "Epoch: 33/100  | Epoch time 49.666 || Average Loss: inf\n",
      "Epoch: 34/100 | Batch 402/402 | Batch time 0.100 || Loss: 4.797898 | loc loss:2.859596 | class loss:1.938303  \n",
      "Epoch: 34/100  | Epoch time 48.633 || Average Loss: inf\n",
      "Epoch: 35/100 | Batch 402/402 | Batch time 0.132 || Loss: 5.445295 | loc loss:3.415941 | class loss:2.029354  \n",
      "Epoch: 35/100  | Epoch time 49.969 || Average Loss: inf\n",
      "Epoch: 36/100 | Batch 402/402 | Batch time 0.094 || Loss: 5.750780 | loc loss:3.755698 | class loss:1.995081  \n",
      "Epoch: 36/100  | Epoch time 48.557 || Average Loss: inf\n",
      "Epoch: 37/100 | Batch 402/402 | Batch time 0.136 || Loss: 6.304938 | loc loss:4.280646 | class loss:2.024292 \n",
      "Epoch: 37/100  | Epoch time 49.845 || Average Loss: inf\n",
      "Epoch: 38/100 | Batch 402/402 | Batch time 0.094 || Loss: 7.073896 | loc loss:5.025424 | class loss:2.048473  \n",
      "Epoch: 38/100  | Epoch time 47.688 || Average Loss: inf\n",
      "Epoch: 39/100 | Batch 402/402 | Batch time 0.098 || Loss: 5.525247 | loc loss:3.612569 | class loss:1.912678 \n",
      "Epoch: 39/100  | Epoch time 46.223 || Average Loss: inf\n",
      "Epoch: 40/100 | Batch 402/402 | Batch time 0.103 || Loss: 6.145791 | loc loss:4.268554 | class loss:1.877236 \n",
      "Epoch: 40/100  | Epoch time 45.400 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_040.h5<<<<<<<<<<\n",
      "Epoch: 41/100 | Batch 402/402 | Batch time 0.128 || Loss: 4.450993 | loc loss:2.325120 | class loss:2.125873 7 \n",
      "Epoch: 41/100  | Epoch time 46.832 || Average Loss: inf\n",
      "Epoch: 42/100 | Batch 402/402 | Batch time 0.114 || Loss: 6.204136 | loc loss:4.229885 | class loss:1.974251  \n",
      "Epoch: 42/100  | Epoch time 45.479 || Average Loss: inf\n",
      "Epoch: 43/100 | Batch 402/402 | Batch time 0.082 || Loss: 5.578299 | loc loss:3.699167 | class loss:1.879131  \n",
      "Epoch: 43/100  | Epoch time 45.646 || Average Loss: inf\n",
      "Epoch: 44/100 | Batch 402/402 | Batch time 0.107 || Loss: 4.782264 | loc loss:2.720629 | class loss:2.061635 \n",
      "Epoch: 44/100  | Epoch time 46.358 || Average Loss: inf\n",
      "Epoch: 45/100 | Batch 402/402 | Batch time 0.086 || Loss: 6.627901 | loc loss:4.580327 | class loss:2.047574 \n",
      "Epoch: 45/100  | Epoch time 46.101 || Average Loss: inf\n",
      "Epoch: 46/100 | Batch 402/402 | Batch time 0.162 || Loss: 5.821097 | loc loss:3.927131 | class loss:1.893966 \n",
      "Epoch: 46/100  | Epoch time 46.925 || Average Loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/100 | Batch 402/402 | Batch time 0.112 || Loss: 6.706421 | loc loss:4.839100 | class loss:1.867321 \n",
      "Epoch: 47/100  | Epoch time 47.138 || Average Loss: inf\n",
      "Epoch: 48/100 | Batch 402/402 | Batch time 0.066 || Loss: 6.060853 | loc loss:4.143591 | class loss:1.917261 \n",
      "Epoch: 48/100  | Epoch time 46.068 || Average Loss: inf\n",
      "Epoch: 49/100 | Batch 402/402 | Batch time 0.077 || Loss: 7.501950 | loc loss:5.477149 | class loss:2.024801  \n",
      "Epoch: 49/100  | Epoch time 46.103 || Average Loss: inf\n",
      "Epoch: 50/100 | Batch 402/402 | Batch time 0.090 || Loss: 7.881362 | loc loss:5.816993 | class loss:2.064369 \n",
      "Epoch: 50/100  | Epoch time 45.634 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_050.h5<<<<<<<<<<\n",
      "Epoch: 51/100 | Batch 402/402 | Batch time 0.119 || Loss: 6.319314 | loc loss:4.378117 | class loss:1.941196 \n",
      "Epoch: 51/100  | Epoch time 46.635 || Average Loss: inf\n",
      "Epoch: 52/100 | Batch 402/402 | Batch time 0.125 || Loss: 7.122972 | loc loss:5.135152 | class loss:1.987820 \n",
      "Epoch: 52/100  | Epoch time 45.695 || Average Loss: inf\n",
      "Epoch: 53/100 | Batch 402/402 | Batch time 0.116 || Loss: 5.271851 | loc loss:3.468108 | class loss:1.803743 \n",
      "Epoch: 53/100  | Epoch time 45.514 || Average Loss: inf\n",
      "Epoch: 54/100 | Batch 402/402 | Batch time 0.099 || Loss: 6.245837 | loc loss:4.330440 | class loss:1.915397 \n",
      "Epoch: 54/100  | Epoch time 44.525 || Average Loss: inf\n",
      "Epoch: 55/100 | Batch 402/402 | Batch time 0.103 || Loss: 4.994118 | loc loss:3.051623 | class loss:1.942495  \n",
      "Epoch: 55/100  | Epoch time 45.320 || Average Loss: inf\n",
      "Epoch: 56/100 | Batch 402/402 | Batch time 0.123 || Loss: 6.633287 | loc loss:4.809973 | class loss:1.823314 \n",
      "Epoch: 56/100  | Epoch time 46.591 || Average Loss: inf\n",
      "Epoch: 57/100 | Batch 402/402 | Batch time 0.113 || Loss: 7.376552 | loc loss:5.334414 | class loss:2.042138  \n",
      "Epoch: 57/100  | Epoch time 45.496 || Average Loss: inf\n",
      "Epoch: 58/100 | Batch 402/402 | Batch time 0.113 || Loss: 7.267800 | loc loss:5.344155 | class loss:1.923645 \n",
      "Epoch: 58/100  | Epoch time 46.980 || Average Loss: inf\n",
      "Epoch: 59/100 | Batch 402/402 | Batch time 0.101 || Loss: 8.014240 | loc loss:6.082513 | class loss:1.931728  \n",
      "Epoch: 59/100  | Epoch time 44.904 || Average Loss: inf\n",
      "Epoch: 60/100 | Batch 402/402 | Batch time 0.094 || Loss: 5.587291 | loc loss:3.702810 | class loss:1.884481 \n",
      "Epoch: 60/100  | Epoch time 46.106 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_060.h5<<<<<<<<<<\n",
      "Epoch: 61/100 | Batch 402/402 | Batch time 0.111 || Loss: 5.501429 | loc loss:3.518687 | class loss:1.982741 \n",
      "Epoch: 61/100  | Epoch time 45.977 || Average Loss: inf\n",
      "Epoch: 62/100 | Batch 402/402 | Batch time 0.107 || Loss: 6.961852 | loc loss:5.179789 | class loss:1.782063  \n",
      "Epoch: 62/100  | Epoch time 45.618 || Average Loss: inf\n",
      "Epoch: 63/100 | Batch 402/402 | Batch time 0.098 || Loss: 5.371668 | loc loss:3.362519 | class loss:2.009150 \n",
      "Epoch: 63/100  | Epoch time 43.944 || Average Loss: inf\n",
      "Epoch: 64/100 | Batch 402/402 | Batch time 0.079 || Loss: 5.261867 | loc loss:3.345939 | class loss:1.915928 \n",
      "Epoch: 64/100  | Epoch time 44.877 || Average Loss: inf\n",
      "Epoch: 65/100 | Batch 402/402 | Batch time 0.121 || Loss: 4.391287 | loc loss:2.512855 | class loss:1.878432 \n",
      "Epoch: 65/100  | Epoch time 45.473 || Average Loss: inf\n",
      "Epoch: 66/100 | Batch 402/402 | Batch time 0.131 || Loss: 6.712395 | loc loss:4.972113 | class loss:1.740281  \n",
      "Epoch: 66/100  | Epoch time 46.348 || Average Loss: inf\n",
      "Epoch: 67/100 | Batch 402/402 | Batch time 0.114 || Loss: 6.308321 | loc loss:4.365662 | class loss:1.942659  \n",
      "Epoch: 67/100  | Epoch time 45.411 || Average Loss: inf\n",
      "Epoch: 68/100 | Batch 402/402 | Batch time 0.089 || Loss: 4.365269 | loc loss:2.290541 | class loss:2.074728  \n",
      "Epoch: 68/100  | Epoch time 45.571 || Average Loss: inf\n",
      "Epoch: 69/100 | Batch 402/402 | Batch time 0.090 || Loss: 5.618870 | loc loss:3.839253 | class loss:1.779616 \n",
      "Epoch: 69/100  | Epoch time 45.812 || Average Loss: inf\n",
      "Epoch: 70/100 | Batch 402/402 | Batch time 0.115 || Loss: 6.163980 | loc loss:4.384669 | class loss:1.779312 \n",
      "Epoch: 70/100  | Epoch time 46.417 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_070.h5<<<<<<<<<<\n",
      "Epoch: 71/100 | Batch 402/402 | Batch time 0.100 || Loss: 6.222840 | loc loss:4.263138 | class loss:1.959703 \n",
      "Epoch: 71/100  | Epoch time 46.497 || Average Loss: inf\n",
      "Epoch: 72/100 | Batch 402/402 | Batch time 0.135 || Loss: 8.222950 | loc loss:6.295187 | class loss:1.927763 \n",
      "Epoch: 72/100  | Epoch time 47.287 || Average Loss: inf\n",
      "Epoch: 73/100 | Batch 402/402 | Batch time 0.072 || Loss: 4.939358 | loc loss:2.910291 | class loss:2.029067 \n",
      "Epoch: 73/100  | Epoch time 45.307 || Average Loss: inf\n",
      "Epoch: 74/100 | Batch 402/402 | Batch time 0.088 || Loss: 7.210047 | loc loss:5.315593 | class loss:1.894454  \n",
      "Epoch: 74/100  | Epoch time 45.830 || Average Loss: inf\n",
      "Epoch: 75/100 | Batch 402/402 | Batch time 0.107 || Loss: 5.657546 | loc loss:3.578173 | class loss:2.079373 \n",
      "Epoch: 75/100  | Epoch time 46.559 || Average Loss: inf\n",
      "Epoch: 76/100 | Batch 402/402 | Batch time 0.133 || Loss: 5.249595 | loc loss:3.345137 | class loss:1.904458 \n",
      "Epoch: 76/100  | Epoch time 49.092 || Average Loss: inf\n",
      "Epoch: 77/100 | Batch 402/402 | Batch time 0.112 || Loss: 4.146734 | loc loss:2.126894 | class loss:2.019840  \n",
      "Epoch: 77/100  | Epoch time 49.154 || Average Loss: inf\n",
      "Epoch: 78/100 | Batch 402/402 | Batch time 0.094 || Loss: 6.000943 | loc loss:3.903730 | class loss:2.097213  \n",
      "Epoch: 78/100  | Epoch time 49.054 || Average Loss: inf\n",
      "Epoch: 79/100 | Batch 402/402 | Batch time 0.100 || Loss: 6.827356 | loc loss:4.934412 | class loss:1.892944 \n",
      "Epoch: 79/100  | Epoch time 47.815 || Average Loss: inf\n",
      "Epoch: 80/100 | Batch 402/402 | Batch time 0.084 || Loss: 5.770782 | loc loss:3.809706 | class loss:1.961076 \n",
      "Epoch: 80/100  | Epoch time 48.340 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_080.h5<<<<<<<<<<\n",
      "Epoch: 81/100 | Batch 402/402 | Batch time 0.061 || Loss: 5.285309 | loc loss:3.259054 | class loss:2.026255 \n",
      "Epoch: 81/100  | Epoch time 49.392 || Average Loss: inf\n",
      "Epoch: 82/100 | Batch 402/402 | Batch time 0.114 || Loss: 7.167593 | loc loss:5.090528 | class loss:2.077065 \n",
      "Epoch: 82/100  | Epoch time 48.021 || Average Loss: inf\n",
      "Epoch: 83/100 | Batch 402/402 | Batch time 0.073 || Loss: 4.590660 | loc loss:2.510705 | class loss:2.079955  \n",
      "Epoch: 83/100  | Epoch time 49.139 || Average Loss: inf\n",
      "Epoch: 84/100 | Batch 402/402 | Batch time 0.111 || Loss: 5.416382 | loc loss:3.499660 | class loss:1.916722  \n",
      "Epoch: 84/100  | Epoch time 48.530 || Average Loss: inf\n",
      "Epoch: 85/100 | Batch 402/402 | Batch time 0.134 || Loss: 5.175393 | loc loss:3.484674 | class loss:1.690719  \n",
      "Epoch: 85/100  | Epoch time 50.528 || Average Loss: inf\n",
      "Epoch: 86/100 | Batch 402/402 | Batch time 0.127 || Loss: 5.061018 | loc loss:3.282574 | class loss:1.778444 \n",
      "Epoch: 86/100  | Epoch time 50.161 || Average Loss: inf\n",
      "Epoch: 87/100 | Batch 402/402 | Batch time 0.110 || Loss: 7.219214 | loc loss:5.202859 | class loss:2.016355  \n",
      "Epoch: 87/100  | Epoch time 49.813 || Average Loss: inf\n",
      "Epoch: 88/100 | Batch 402/402 | Batch time 0.103 || Loss: 6.049969 | loc loss:4.113919 | class loss:1.936050 \n",
      "Epoch: 88/100  | Epoch time 49.362 || Average Loss: inf\n",
      "Epoch: 89/100 | Batch 402/402 | Batch time 0.071 || Loss: 6.373643 | loc loss:4.519361 | class loss:1.854282 \n",
      "Epoch: 89/100  | Epoch time 49.091 || Average Loss: inf\n",
      "Epoch: 90/100 | Batch 402/402 | Batch time 0.078 || Loss: 6.596292 | loc loss:4.699574 | class loss:1.896718  \n",
      "Epoch: 90/100  | Epoch time 48.029 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_090.h5<<<<<<<<<<\n",
      "Epoch: 91/100 | Batch 402/402 | Batch time 0.119 || Loss: 5.750369 | loc loss:3.725453 | class loss:2.024915 \n",
      "Epoch: 91/100  | Epoch time 48.854 || Average Loss: inf\n",
      "Epoch: 92/100 | Batch 402/402 | Batch time 0.093 || Loss: 6.459609 | loc loss:4.624495 | class loss:1.835114 \n",
      "Epoch: 92/100  | Epoch time 48.171 || Average Loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/100 | Batch 402/402 | Batch time 0.116 || Loss: 7.764812 | loc loss:5.726234 | class loss:2.038578  \n",
      "Epoch: 93/100  | Epoch time 48.378 || Average Loss: inf\n",
      "Epoch: 94/100 | Batch 402/402 | Batch time 0.115 || Loss: 8.018726 | loc loss:6.003720 | class loss:2.015007  \n",
      "Epoch: 94/100  | Epoch time 48.953 || Average Loss: inf\n",
      "Epoch: 95/100 | Batch 402/402 | Batch time 0.107 || Loss: 5.895450 | loc loss:4.007250 | class loss:1.888199  \n",
      "Epoch: 95/100  | Epoch time 46.017 || Average Loss: inf\n",
      "Epoch: 96/100 | Batch 402/402 | Batch time 0.087 || Loss: 6.343213 | loc loss:4.646858 | class loss:1.696354 \n",
      "Epoch: 96/100  | Epoch time 46.830 || Average Loss: inf\n",
      "Epoch: 97/100 | Batch 402/402 | Batch time 0.094 || Loss: 6.074240 | loc loss:4.083025 | class loss:1.991215 \n",
      "Epoch: 97/100  | Epoch time 47.074 || Average Loss: inf\n",
      "Epoch: 98/100 | Batch 402/402 | Batch time 0.088 || Loss: 5.176002 | loc loss:3.338637 | class loss:1.837365  \n",
      "Epoch: 98/100  | Epoch time 46.295 || Average Loss: inf\n",
      "Epoch: 99/100 | Batch 402/402 | Batch time 0.119 || Loss: 5.879157 | loc loss:4.029332 | class loss:1.849825 \n",
      "Epoch: 99/100  | Epoch time 45.230 || Average Loss: inf\n",
      "Epoch: 100/100 | Batch 402/402 | Batch time 0.118 || Loss: 6.230939 | loc loss:4.168341 | class loss:2.062599  \n",
      "Epoch: 100/100  | Epoch time 45.345 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_100.h5<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(init_epoch+1,cfg['epoch']):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        avg_loss = 0.0\n",
    "        for step, (inputs, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "\n",
    "            load_t0 = time.time()\n",
    "            total_loss, losses = train_step(inputs, labels)\n",
    "            avg_loss = (avg_loss * step + total_loss.numpy()) / (step + 1)\n",
    "            load_t1 = time.time()\n",
    "            batch_time = load_t1 - load_t0\n",
    "\n",
    "            steps =steps_per_epoch*epoch+step\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss/total_loss', total_loss, step=steps)\n",
    "                for k, l in losses.items():\n",
    "                    tf.summary.scalar('loss/{}'.format(k), l, step=steps)\n",
    "                tf.summary.scalar('learning_rate', optimizer.lr(steps), step=steps)\n",
    "\n",
    "            print(f\"\\rEpoch: {epoch + 1}/{cfg['epoch']} | Batch {step + 1}/{steps_per_epoch} | Batch time {batch_time:.3f} || Loss: {total_loss:.6f} | loc loss:{losses['loc']:.6f} | class loss:{losses['class']:.6f} \",end = '',flush=True)\n",
    "\n",
    "        print(f\"\\nEpoch: {epoch + 1}/{cfg['epoch']}  | Epoch time {(load_t1 - start):.3f} || Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/avg_loss',avg_loss,step=epoch)\n",
    "\n",
    "        if (epoch + 1) % cfg['save_freq'] == 0:\n",
    "            filepath = os.path.join(weights_dir, f'weights_epoch_{(epoch + 1):03d}.h5')\n",
    "            model.save_weights(filepath)\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\">>>>>>>>>>Save weights file at {filepath}<<<<<<<<<<\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22fff0",
   "metadata": {},
   "source": [
    "이번 스텝에 소개한 모델 학습 과정을 train.py에 정리해 두었습니다.   \n",
    "아래와 같이 위의 과정을 실행할 수 있습니다.\n",
    "\n",
    "` cd ~/aiffel/face_detector && python train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2143ca",
   "metadata": {},
   "source": [
    "## 7. Inference(1) NMS\n",
    "\n",
    "\n",
    "### NMS 구현하기\n",
    "\n",
    "Grid cell을 사용하는 Object detection의 inference 단계에서 <span style='color:red'> 하나의 object가 여러 개의 prior box에 걸쳐져 있을 때 가장 확률이 높은 1개의 prior box를 하나로 줄여주는</span> **NMS(non-max suppression)** 이 필요합니다. 아래 코드를 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b7abe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bbox_tf(pre, priors, variances=None):\n",
    "    \"\"\"Decode locations from predictions using prior to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): location predictions for loc layers,\n",
    "            Shape: [num_prior,4]\n",
    "        prior (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_prior,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        decoded bounding box predictions xmin, ymin, xmax, ymax\n",
    "    \"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]\n",
    "    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])\n",
    "\n",
    "    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f51d5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nms(boxes, scores, nms_threshold=0.5, limit=200):\n",
    "    \"\"\" Perform Non Maximum Suppression algorithm\n",
    "        to eliminate boxes with high overlap\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        scores: tensor (num_boxes,)\n",
    "        nms_threshold: NMS threshold\n",
    "        limit: maximum number of boxes to keep\n",
    "    Returns:\n",
    "        idx: indices of kept boxes\n",
    "    \"\"\"\n",
    "    if boxes.shape[0] == 0:\n",
    "        return tf.constant([], dtype=tf.int32)\n",
    "    selected = [0]\n",
    "    idx = tf.argsort(scores, direction='DESCENDING')\n",
    "    idx = idx[:limit]\n",
    "    boxes = tf.gather(boxes, idx)\n",
    "\n",
    "    iou = _jaccard(boxes, boxes)\n",
    "\n",
    "    while True:\n",
    "        row = iou[selected[-1]]\n",
    "        next_indices = row <= nms_threshold\n",
    "\n",
    "        iou = tf.where(\n",
    "            tf.expand_dims(tf.math.logical_not(next_indices), 0),\n",
    "            tf.ones_like(iou, dtype=tf.float32),\n",
    "            iou)\n",
    "\n",
    "        if not tf.math.reduce_any(next_indices):\n",
    "            break\n",
    "\n",
    "        selected.append(tf.argsort(\n",
    "            tf.dtypes.cast(next_indices, tf.int32), direction='DESCENDING')[0].numpy())\n",
    "\n",
    "    return tf.gather(idx, selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b44d68",
   "metadata": {},
   "source": [
    "* `tf.argsort()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cb358d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_predict(predictions, priors, cfg):\n",
    "    label_classes = cfg['labels_list']\n",
    "\n",
    "    bbox_regressions, confs = tf.split(predictions[0], [4, -1], axis=-1)\n",
    "    boxes = decode_bbox_tf(bbox_regressions, priors, cfg['variances'])\n",
    "\n",
    "\n",
    "    confs = tf.math.softmax(confs, axis=-1)\n",
    "\n",
    "    out_boxes = []\n",
    "    out_labels = []\n",
    "    out_scores = []\n",
    "\n",
    "    for c in range(1, len(label_classes)):\n",
    "        cls_scores = confs[:, c]\n",
    "\n",
    "        score_idx = cls_scores > cfg['score_threshold']\n",
    "\n",
    "        cls_boxes = boxes[score_idx]\n",
    "        cls_scores = cls_scores[score_idx]\n",
    "\n",
    "        nms_idx = compute_nms(cls_boxes, cls_scores, cfg['nms_threshold'], cfg['max_number_keep'])\n",
    "\n",
    "        cls_boxes = tf.gather(cls_boxes, nms_idx)\n",
    "        cls_scores = tf.gather(cls_scores, nms_idx)\n",
    "\n",
    "        cls_labels = [c] * cls_boxes.shape[0]\n",
    "\n",
    "        out_boxes.append(cls_boxes)\n",
    "        out_labels.extend(cls_labels)\n",
    "        out_scores.append(cls_scores)\n",
    "\n",
    "    out_boxes = tf.concat(out_boxes, axis=0)\n",
    "    out_scores = tf.concat(out_scores, axis=0)\n",
    "\n",
    "    boxes = tf.clip_by_value(out_boxes, 0.0, 1.0).numpy()\n",
    "    classes = np.array(out_labels)\n",
    "    scores = out_scores.numpy()\n",
    "\n",
    "    return boxes, classes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfe99d",
   "metadata": {},
   "source": [
    "* `tf.math.softmax()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb086f",
   "metadata": {},
   "source": [
    "## 8. Inference(2) 사진에서 얼굴 찾기\n",
    "\n",
    "\n",
    "### 사진에서 여러개의 얼굴을 찾아보자.\n",
    "\n",
    "이제 다 왔습니다. SSD 모델을 통해 우리는 **Multi-face detection** 기능을 확보했습니다.\n",
    "얼마나 잘 해내는지 확인해 보도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e70a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input_image(img, max_steps):\n",
    "    \"\"\"pad image to suitable shape\"\"\"\n",
    "    img_h, img_w, _ = img.shape\n",
    "\n",
    "    img_pad_h = 0\n",
    "    if img_h % max_steps > 0:\n",
    "        img_pad_h = max_steps - img_h % max_steps\n",
    "\n",
    "    img_pad_w = 0\n",
    "    if img_w % max_steps > 0:\n",
    "        img_pad_w = max_steps - img_w % max_steps\n",
    "\n",
    "    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n",
    "    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n",
    "                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n",
    "    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n",
    "\n",
    "    return img, pad_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5eba11",
   "metadata": {},
   "source": [
    "* `cv2.copyMakeBorder()` :\n",
    "* `cv2.BORDER_CONSTANT` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a11cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_pad_output(outputs, pad_params):\n",
    "    \"\"\"\n",
    "        recover the padded output effect\n",
    "\n",
    "    \"\"\"\n",
    "    img_h, img_w, img_pad_h, img_pad_w = pad_params\n",
    "\n",
    "    recover_xy = np.reshape(outputs[0], [-1, 2, 2]) * \\\n",
    "                 [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n",
    "    outputs[0] = np.reshape(recover_xy, [-1, 4])\n",
    "\n",
    "    return outputs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "602b8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, boxes, classes, scores, img_height, img_width, prior_index, class_list):\n",
    "    \"\"\"\n",
    "    draw bboxes and labels\n",
    "    out:boxes,classes,scores\n",
    "    \"\"\"\n",
    "    # bbox\n",
    "\n",
    "    x1, y1, x2, y2 = int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height), \\\n",
    "                     int(boxes[prior_index][2] * img_width), int(boxes[prior_index][3] * img_height)\n",
    "    if classes[prior_index] == 1:\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        color = (0, 0, 255)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "    \n",
    "    # confidence\n",
    "\n",
    "#     if scores:\n",
    "#         score = \"{:.4f}\".format(scores[prior_index])\n",
    "#         class_name = class_list[classes[prior_index]]\n",
    "\n",
    "    score = \"{:.4f}\".format(scores[prior_index])\n",
    "    class_name = class_list[classes[prior_index]]\n",
    "    \n",
    "    cv2.putText(img, '{} {}'.format(class_name, score),\n",
    "                (int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height) - 4),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e9e47",
   "metadata": {},
   "source": [
    "inference와 화면출력을 위한 몇가지 기능을 추가하였습니다.\n",
    "\n",
    "여러 사람의 얼굴이 포함된 테스트용 이미지를 골라 주세요.  \n",
    "`~/aiffel/face_detector/image.png`라는 경로로 저장 후 아래 코드를 실행해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9146a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path : /home/aiffel-d34j/aiffel/face_detector/checkpoints/weights_epoch_100.h5\n",
      "[*] Predict /home/aiffel-d34j/aiffel/face_detector/image.png image.. \n",
      "(256, 320, 3)\n",
      "scores:[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-e07ff1ede8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# recover padding effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecover_pad_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# draw and save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-62da4d702043>\u001b[0m in \u001b[0;36mrecover_pad_output\u001b[0;34m(outputs, pad_params)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_pad_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_pad_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrecover_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                  \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_pad_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_pad_h\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimg_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecover_xy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "global model\n",
    "\n",
    "min_sizes = cfg['min_sizes']\n",
    "num_cell = [len(min_sizes[k]) for k in range(len(cfg['steps']))]\n",
    "model_path = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=False)\n",
    "\n",
    "    paths = [os.path.join(model_path, path)\n",
    "             for path in os.listdir(model_path)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    print(f\"model path : {latest}\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print('Please make sure there is at least one weights at {}'.format(model_path))\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    print(f\"Cannot find image path from {img_path}\")\n",
    "    exit()\n",
    "    \n",
    "print(\"[*] Predict {} image.. \".format(img_path))\n",
    "img_raw = cv2.imread(img_path)\n",
    "img_raw = cv2.resize(img_raw, (320, 240))\n",
    "img_height_raw, img_width_raw, _ = img_raw.shape\n",
    "img = np.float32(img_raw.copy())\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# pad input image to avoid unmatched shape problem\n",
    "img, pad_params = pad_input_image(img, max_steps=max(cfg['steps']))\n",
    "img = img / 255.0 - 0.5\n",
    "print(img.shape)\n",
    "\n",
    "priors, _ = prior_box(cfg, image_sizes=(img.shape[0], img.shape[1]))\n",
    "priors = tf.cast(priors, tf.float32)\n",
    "\n",
    "predictions = model.predict(img[np.newaxis, ...])\n",
    "\n",
    "boxes, classes, scores = parse_predict(predictions, priors, cfg)\n",
    "\n",
    "print(f\"scores:{scores}\")\n",
    "\n",
    "# recover padding effect\n",
    "boxes = recover_pad_output(boxes, pad_params)\n",
    "\n",
    "# draw and save results\n",
    "save_img_path = os.getenv('HOME') + os.path.join('face_detector/assets/out_' + os.path.basename(img_path))\n",
    "\n",
    "for prior_index in range(len(boxes)):\n",
    "    show_image(img_raw, boxes, classes, scores, img_height_raw, img_width_raw, prior_index, cfg['labels_list'])\n",
    "\n",
    "cv2.imwrite(save_img_path, img_raw)\n",
    "cv2.imshow('results', img_raw)\n",
    "\n",
    "if cv2.waitKey(0) == ord('q'):\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f32e7",
   "metadata": {},
   "source": [
    "이번 스텝에 소개한 모델 학습 과정을 `inference.py`에 정리해 두었습니다.   \n",
    "아래와 같이 위의 과정을 실행할 수 있습니다.\n",
    "(단, 이미지 파일명은 임의로 변경할 수 있습니다. )\n",
    "\n",
    "` cd ~/aiffel/face_detector && python inference.py  checkpoints/  image.png `\n",
    "\n",
    "어떤가요? 결과가 잘 나오시나요? 저는 이렇게 결과가 나왔습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc49b06",
   "metadata": {},
   "source": [
    "` cd ~/aiffel/face_detector && python tf_dataset_preprocess.py\n",
    " cd ~/aiffel/face_detector && python tf_build_ssd_model.py\n",
    " cd ~/aiffel/face_detector && python tf_dataloader.py\n",
    " cd ~/aiffel/face_detector && python train.py\n",
    " cd ~/aiffel/face_detector && python inference.py  checkpoints/  image.png `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723835e",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998dba98",
   "metadata": {},
   "source": [
    "* 텐서플로우-GPU 버전 2.3.0을 새로운 가상환경에 설치하면 GPU 기능이 활성화 되지 않았다.\n",
    "  - pip install tensorflow-gpu==2.3.0으로 패키지를 설치하면 GPU 가 적용되지 않았다.\n",
    "  - cuda와 cuDNN이 정상적으로 설치되어 nvidia-smi에서도 잘 확인되는데 왜 GPU가 활성화 되지 않는지 모르고 있다.\n",
    "  - conda install tensorflow-gpu==2.3.0이 정상적으로 설치되지 않았다.\n",
    "  - 버전을 낮추어 conda install tensorflow-gpu==2.2.0으로 하였을 경우에 GPU가 정상적으로 적용되었다.\n",
    "\n",
    "* 여러개의 파이썬 함수들이 있었는데 각 함수에 사용된 내장함수들을 제대로 알아보지 못한것이 아쉽다.\n",
    "\n",
    "* 실질적으로 적용하는 것이 잘 되지 못해서 내용을 다시 분석해봐야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32974d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py37tf23",
   "language": "python",
   "name": "py37tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
